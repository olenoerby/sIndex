POSTGRES_USER=pineapple
POSTGRES_PASSWORD=pineapple
POSTGRES_DB=pineapple
POSTGRES_HOST=db
POSTGRES_PORT=5432
DATABASE_URL=postgresql+psycopg2://pineapple:pineapple@db:5432/pineapple
REDIS_URL=redis://redis:6379/0

# API key for protected endpoints (e.g., /subreddits/{name}/refresh)
# Generate a new one with: python -c "import secrets; print(secrets.token_urlsafe(32))"
API_KEY=your-api-key-here

# Timezone for all containers
TZ=Europe/Copenhagen

# Log level for scanner (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# How many API requests to allow per minute to avoid rate limiting
API_MAX_CALLS_MINUTE=8

# Seconds to spend refreshing metadata after each scan completes (before next scan)
METADATA_REFRESH_SECONDS=7200

# Start with metadata refresh on scanner startup (true/false)
SCAN_FOR_METADATA_FIRST=true

# Testing helper variables (to limit scope during tests)
# Maximum number of posts to process per source subreddit (e.g., 2 posts from nsfw411 AND 2 from wowthissubexists)
TEST_MAX_POSTS_PER_SUBREDDIT=

# How many days back to rescan existing posts for new/edited comments.
POST_COMMENT_LOOKBACK_DAYS=

# Skip posts that were scanned within the last X hours (useful for container restarts)
# Set to 0 to disable this feature and always scan posts.
SKIP_RECENTLY_SCANNED_HOURS=24

# How many seconds to sleep between scan iterations (after metadata refresh completes)
SCAN_SLEEP_SECONDS=300

# NOTE: Scan configuration is now managed in the database via these tables:
# - subreddit_scan_configs: which subreddits to scan and how (users, NSFW filter)
# - ignored_subreddits: subreddits to never record mentions for
# - ignored_users: users whose mentions should not be recorded
# Use the database to manage scan targets instead of .env variables.

# Runtime/config values
# Maximum number of retry attempts when fetching subreddit metadata from Reddit's /about.json endpoint
SUBABOUT_MAX_RETRIES=3
# Timeout in seconds for HTTP requests to Reddit API (prevents hanging on slow/failed connections)
HTTP_REQUEST_TIMEOUT=15

# How many hours before metadata is considered stale and needs refreshing
METADATA_STALE_HOURS=24

# Redis cache TTL (time-to-live) in seconds for API responses
# Controls how long API responses are cached before being refreshed
CACHE_TTL_DEFAULT=30       # General endpoints (subreddit listings, search results)
CACHE_TTL_STATS=60         # Statistics endpoints (counts, aggregations)
CACHE_TTL_ANALYTICS=300    # Analytics endpoints (5 minutes - complex queries, slower to change)

# Set to true, to add initial scan configuration on container start to the database.
# This is idempotent and safe to run multiple times.
INIT_SCAN_CONFIG=true

# Optional: Cloudflare Tunnel Token (if using cloudflared service in docker-compose)
# Generate from: https://one.dash.cloudflare.com/ -> Zero Trust -> Networks -> Tunnels
# CLOUDFLARE_TUNNEL_TOKEN=your_tunnel_token_here
